# gRPC load balancing

> gRPC предпочитает **тонкий client‑side load balancing**: клиент знает несколько адресов бэкендов и сам распределяет запросы между ними.  
> Вокруг этого можно строить разные схемы с L4/L7 балансировщиками, service discovery и сервис‑мэшами.

---

## Модели балансировки для gRPC

В реальном мире обычно комбинируются несколько подходов:

- **L4‑балансировщик**  
  Балансировка на уровне соединений (TCP). Хорошо знакомый паттерн, но с HTTP/2 и gRPC одно соединение может обслуживать много запросов — в итоге балансируется не нагрузка, а **количество соединений**.
- **L7‑балансировщик (прокси)**  
  Envoy/NGINX/Ingress‑контроллер принимают HTTP/2/gRPC, сами держат коннекты к бэкендам и уже балансируют запросы. Клиенты видят одну точку входа.
- **Client‑side load balancing**  
  Клиент получает список эндпоинтов (через DNS, service discovery или от мэша) и сам распределяет запросы по серверам (round‑robin, weighted, least‑request и т.д.).

Основная рекомендация: **для gRPC желательно балансировать на уровне запросов**, а не только коннектов, иначе один клиент может «прилипнуть» к одному серверу.

---

## Базовая схема с L4‑балансировщиком

Минимальный вариант, когда «просто нужно, чтобы работало»:

- несколько инстансов gRPC‑сервиса;
- L4‑балансировщик (Kubernetes `Service` типа [translate:LoadBalancer], NLB, TCP‑балансировщик в облаке);
- каждый клиент подключается к `lb:port` как к одному адресу.

Плюсы:

- просто настроить;
- подходит для начала и небольших нагрузок.

Минусы:

- балансировка по соединениям: клиент установил одно HTTP/2‑соединение — все запросы уйдут на один бэкенд;
- при длинных коннектах распределение нагрузки может быть очень неравномерным.

---

## Client‑side load balancing в Go

В `grpc-go` есть встроенная поддержка клиентской балансировки. Схема:

1. Клиент получает список адресов (обычно через DNS или service discovery).
2. Открывает отдельное соединение к каждому адресату.
3. Балансировщик внутри клиента выбирает `SubConn` для каждого RPC.

Пример подключения round‑robin через service config:

```go
    conn, err := grpc.Dial(
        "dns:///my-headless-service.default.svc.cluster.local:50051",
        grpc.WithTransportCredentials(creds),
        grpc.WithDefaultServiceConfig(`{"loadBalancingPolicy":"round_robin"}`),
        )
        if err != nil {
        // ...
        }
		defer conn.Close()

    client := usersv1.NewUserServiceClient(conn)
```

Здесь:

- `dns:///` — схема, при которой клиент получит **список IP‑адресов** по DNS‑имени (подходит для headless‑сервиса в Kubernetes);
- `loadBalancingPolicy: "round_robin"` — клиент будет равномерно раскидывать запросы по всем активным соединениям.

Другие стандартные политики: [translate:pick_first], [translate:round_robin], более продвинутые (ring‑hash, weighted, least‑request) доступны как отдельные реализации.

---

## L7‑балансировщик / сервис‑мэш

Если в инфраструктуре уже есть Envoy/Ingress/Service Mesh (Istio, Linkerd и т.д.):

- gRPC‑клиент подключается к локальному sidecar‑прокси (или к общему L7‑прокси);
- прокси:
  - делает health‑checks;
  - знает список бэкендов;
  - балансирует запросы;
  - реализует retry, rate‑limit, circuit‑breaking.

Плюсы:

- тонкий клиент, вся сложность — в инфраструктуре;
- централизованные политики, единые для разных языков и сервисов.

Минусы:

- дополнительный уровень сложности и латентности;
- зависимость от конкретного мэша/прокси.

---

## Связка load balancing + health‑checks

Балансировка должна учитывать только **здоровые** инстансы:

- на стороне прокси / L7‑балансировщика:
  - он делает gRPC‑health‑check или HTTP‑пробы до каждого бэкенда;
  - исключает *NOT_SERVING* инстансы из пулла;
- при client‑side LB:
  - источники адресов (DNS, discovery) выдают только живые инстансы;
  - либо клиент периодически переподключается и обновляет список.

Хорошая практика:

- использовать **стандартный** gRPC health‑service;
- настроить балансировщик/мэш на работу с этим health‑чеком;
- на стороне сервиса управлять статусом (готов / не готов) при смене состояния зависимостей.

---

## Нюансы с долгими соединениями и стримингом

При стриминговых RPC:

- один стрим привязан к конкретному соединению и инстансу сервера;
- балансировка происходит **между стримами**, а не внутри одного стрима;
- для долгоживущих стримов важно:
  - иметь достаточно инстансов;
  - учитывать, что один «толстый» стрим может нагрузить один сервер сильнее остальных.

При client‑side LB полезно:

- на сервере ограничивать максимальный возраст соединения (MaxConnectionAge), чтобы клиенты периодически переподключались и «видели» новые инстансы;
- на клиенте включить повторное разрешение адресов, если это поддерживается (DNS, discovery).

---

## Когда что использовать

- **Простой кейс / маленький проект**  
  L4‑балансировщик + один `grpc.Dial("lb:port")`. Понять, хватает ли.

- **Нужна ровная балансировка и масштабирование**  
  Client‑side LB (DNS/headless‑service + round‑robin) или L7‑прокси с gRPC‑поддержкой.

- **Большой зоопарк сервисов и языков**  
  Сервис‑мэш: балансировка, health, retry, mtls и наблюдаемость в одном месте.

Главное — помнить, что с gRPC и HTTP/2 **балансировать только TCP‑коннекты часто недостаточно**, а правильная схема — это комбинация health‑checks, грамотного discovery и request‑level балансировки.
